{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNa+HVXvLPKEGi7nnx6QEwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkoMile/mlp-from-scratch/blob/master/MLP_minibatch_grad_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I am trying to create a Multi-layer perceptron from scratch, meaning I will only use numPy for creating and training the NN.\n",
        "\n",
        "The goal with this project is to learn how a MLP \"learns\" and to understand the calculus behind basic concepts in deep learning, like gradient descent and back-propagation."
      ],
      "metadata": {
        "id": "imE2DcBPsHBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "BrrPlEPTsgKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training and testing arrays\n",
        "\n",
        "(x_train,y_train), (x_test,y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# normalize pixel values to 0-1\n",
        "\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "id": "4XAbiwI9swLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "HIDDEN_LAYER_NEURONS = 16\n",
        "LEARN_RATE = 0.01  # Reduced learning rate\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-(x)))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def cost_fn(y_hat,y):\n",
        "  return (y_hat-y)**2\n",
        "\n",
        "def d_cost_fn(y_hat,y):\n",
        "  return 2*(y_hat-y)"
      ],
      "metadata": {
        "id": "1Yne1P37yXAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create variables\n",
        "\n",
        "# we are creating a 1 hidden layer MLP\n",
        "hidden_layer_activation = np.random.rand(HIDDEN_LAYER_NEURONS)\n",
        "hidden_layer_z = np.random.rand(HIDDEN_LAYER_NEURONS)\n",
        "\n",
        "# neuron biases\n",
        "hidden_biases = np.random.randn(HIDDEN_LAYER_NEURONS) * 0.01\n",
        "output_biases = np.random.randn(10) * 0.01\n",
        "\n",
        "# weight matrix\n",
        "input_weights = np.random.randn(HIDDEN_LAYER_NEURONS,784) * 0.01\n",
        "output_weights = np.random.randn(10,HIDDEN_LAYER_NEURONS) * 0.01\n",
        "\n",
        "# slopes\n",
        "slope_w0 = np.zeros((784,HIDDEN_LAYER_NEURONS))\n",
        "slope_w1 = np.zeros((HIDDEN_LAYER_NEURONS,10))\n",
        "slope_b0 = np.zeros(HIDDEN_LAYER_NEURONS)\n",
        "slope_b1 = np.zeros(10)\n",
        "\n",
        "# output layer\n",
        "y_hat = np.zeros(10)\n",
        "y_hat_z = np.zeros(10)"
      ],
      "metadata": {
        "id": "bGIrf29ewiHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feedforward(input_layer):\n",
        "    hidden_layer_z = np.dot(input_weights, input_layer) + hidden_biases\n",
        "    hidden_layer_activation = sigmoid(hidden_layer_z)\n",
        "    y_hat_z = np.dot(output_weights, hidden_layer_activation) + output_biases\n",
        "    y_hat = sigmoid(y_hat_z)\n",
        "    return hidden_layer_activation, y_hat_z, y_hat\n",
        "\n",
        "def backpropagation(input_layer, hidden_layer_activation, y_hat_z, y_hat, y):\n",
        "    slope_b1 = d_cost_fn(y_hat, y) * d_sigmoid(y_hat_z)\n",
        "    slope_w1 = np.dot(slope_b1.reshape(10, 1), hidden_layer_activation.reshape(1, HIDDEN_LAYER_NEURONS))\n",
        "    slope_b0 = np.dot(output_weights.T, slope_b1) * d_sigmoid(np.dot(input_weights, input_layer) + hidden_biases)\n",
        "    slope_w0 = np.dot(slope_b0.reshape(HIDDEN_LAYER_NEURONS, 1), input_layer.reshape(1, 784))\n",
        "    return slope_w0, slope_w1, slope_b0, slope_b1\n",
        "\n",
        "def update_params(slope_w0, slope_w1, slope_b0, slope_b1, learn_rate):\n",
        "    global input_weights, output_weights, hidden_biases, output_biases\n",
        "    input_weights -= learn_rate * slope_w0\n",
        "    output_weights -= learn_rate * slope_w1\n",
        "    hidden_biases -= learn_rate * slope_b0\n",
        "    output_biases -= learn_rate * slope_b1"
      ],
      "metadata": {
        "id": "nEDQEMoipWB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "# Training\n",
        "cost_minibatch_arr = []\n",
        "cost_epoch_arr = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    #shuffling the dataset\n",
        "    indices = np.random.permutation(x_train.shape[0])\n",
        "    x_train_shuffled = x_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "\n",
        "    for start in range(0, x_train.shape[0], BATCH_SIZE):\n",
        "        end = start + BATCH_SIZE\n",
        "        batch_x = x_train_shuffled[start:end]\n",
        "        batch_y = y_train_shuffled[start:end]\n",
        "\n",
        "        # initialize to 0, because they will be summed\n",
        "        batch_slope_w0 = np.zeros(input_weights.shape)\n",
        "        batch_slope_w1 = np.zeros(output_weights.shape)\n",
        "        batch_slope_b0 = np.zeros(hidden_biases.shape)\n",
        "        batch_slope_b1 = np.zeros(output_biases.shape)\n",
        "\n",
        "        batch_cost = 0\n",
        "\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            input_layer = batch_x[i].reshape(784)\n",
        "            hidden_layer_activation, y_hat_z, y_hat = feedforward(input_layer)\n",
        "            y = np.zeros(10)\n",
        "            y[batch_y[i]] = 1\n",
        "            batch_cost += np.sum(cost_fn(y_hat, y))\n",
        "\n",
        "            slope_w0, slope_w1, slope_b0, slope_b1 = backpropagation(\n",
        "                input_layer, hidden_layer_activation, y_hat_z, y_hat, y\n",
        "            )\n",
        "            batch_slope_w0 += slope_w0\n",
        "            batch_slope_w1 += slope_w1\n",
        "            batch_slope_b0 += slope_b0\n",
        "            batch_slope_b1 += slope_b1\n",
        "\n",
        "        # update the params for the average deltas of the mini-batch\n",
        "        update_params(batch_slope_w0 / BATCH_SIZE, batch_slope_w1 / BATCH_SIZE,\n",
        "                      batch_slope_b0 / BATCH_SIZE, batch_slope_b1 / BATCH_SIZE,\n",
        "                      LEARN_RATE)\n",
        "\n",
        "        cost_minibatch_arr.append(batch_cost / BATCH_SIZE)\n",
        "    cost_epoch_arr.append(np.sum(cost_minibatch_arr) / len(cost_minibatch_arr))\n",
        "    cost_minibatch_arr = []\n",
        "\n",
        "plt.plot(cost_epoch_arr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4wBo_xup6XHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "test_results = []\n",
        "for i in range(x_test.shape[0]):\n",
        "    input_layer = x_test[i].reshape(784)\n",
        "    _, _, y_hat = feedforward(input_layer)\n",
        "    y = y_test[i]\n",
        "    test_results.append((np.argmax(y_hat), y))\n",
        "\n",
        "accuracy = sum(int(x == y) for x, y in test_results) / y_test.shape[0] * 100\n",
        "print(f'{accuracy}% accuracy')"
      ],
      "metadata": {
        "id": "zRfqTT9XSlm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}