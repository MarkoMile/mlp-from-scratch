{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvL8e9tn5EExncz/r1gFoy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkoMile/mlp-from-scratch/blob/master/MLP_grad_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I am trying to create a Multi-layer perceptron from scratch, meaning I will only use numPy for creating and training the NN.\n",
        "\n",
        "The goal with this project is to learn how a MLP \"learns\" and to understand the calculus behind basic concepts in deep learning, like gradient descent and back-propagation."
      ],
      "metadata": {
        "id": "imE2DcBPsHBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "BrrPlEPTsgKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training and testing arrays\n",
        "\n",
        "(x_train,y_train), (x_test,y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# normalize pixel values to 0-1\n",
        "\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0"
      ],
      "metadata": {
        "id": "4XAbiwI9swLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "HIDDEN_LAYER_NEURONS = 16\n",
        "EPOCHS = 5\n",
        "TRAINING_SIZE = 60000\n",
        "LEARN_RATE = 0.01  # Reduced learning rate\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-(x)))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def cost_fn(y_hat,y):\n",
        "  return (y_hat-y)**2\n",
        "\n",
        "def d_cost_fn(y_hat,y):\n",
        "  return 2*(y_hat-y)"
      ],
      "metadata": {
        "id": "1Yne1P37yXAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create variables\n",
        "\n",
        "# we are creating a 1 hidden layer MLP\n",
        "hidden_layer_activation = np.random.rand(HIDDEN_LAYER_NEURONS)\n",
        "hidden_layer_z = np.random.rand(HIDDEN_LAYER_NEURONS)\n",
        "\n",
        "# neuron biases\n",
        "hidden_biases = np.random.randn(HIDDEN_LAYER_NEURONS) * 0.01\n",
        "output_biases = np.random.randn(10) * 0.01\n",
        "\n",
        "# weight matrix\n",
        "input_weights = np.random.randn(HIDDEN_LAYER_NEURONS,784) * 0.01\n",
        "output_weights = np.random.randn(10,HIDDEN_LAYER_NEURONS) * 0.01\n",
        "\n",
        "# slopes\n",
        "slope_w0 = np.zeros((784,HIDDEN_LAYER_NEURONS))\n",
        "slope_w1 = np.zeros((HIDDEN_LAYER_NEURONS,10))\n",
        "slope_b0 = np.zeros(HIDDEN_LAYER_NEURONS)\n",
        "slope_b1 = np.zeros(10)\n",
        "\n",
        "# output layer\n",
        "y_hat = np.zeros(10)\n",
        "y_hat_z = np.zeros(10)"
      ],
      "metadata": {
        "id": "bGIrf29ewiHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "cost = 0\n",
        "cost_arr = []\n",
        "epoch_cost_arr = []\n",
        "\n",
        "for j in range(EPOCHS):\n",
        "  for i in tqdm(range(0,TRAINING_SIZE)):\n",
        "    # feed forward\n",
        "    input_layer = x_train[i].reshape(784)\n",
        "\n",
        "    hidden_layer_z = np.dot(input_weights,input_layer)+hidden_biases\n",
        "    hidden_layer_activation = sigmoid(hidden_layer_z)\n",
        "\n",
        "    y_hat_z = np.dot(output_weights,hidden_layer_activation)+output_biases\n",
        "    y_hat = sigmoid(y_hat_z)\n",
        "\n",
        "    y = np.zeros(10)\n",
        "    y[y_train[i]] = 1\n",
        "\n",
        "    cost = cost_fn(y_hat,y)\n",
        "\n",
        "    # backpropagation\n",
        "    delta = d_cost_fn(y_hat,y)*d_sigmoid(y_hat_z)\n",
        "\n",
        "    slope_b1 = delta\n",
        "    slope_w1 = np.dot(delta.reshape(10,1),hidden_layer_activation.reshape(1,16))\n",
        "\n",
        "    delta = np.dot(delta,output_weights)*d_sigmoid(hidden_layer_z)\n",
        "\n",
        "    slope_b0 = delta\n",
        "    slope_w0 = np.dot(delta.reshape(16,1),input_layer.reshape(1,784))\n",
        "\n",
        "    hidden_biases -= slope_b0 * LEARN_RATE\n",
        "    output_biases -= slope_b1 * LEARN_RATE\n",
        "    input_weights -= slope_w0 * LEARN_RATE\n",
        "    output_weights -= slope_w1 * LEARN_RATE\n",
        "\n",
        "    cost_arr.append(np.sum(cost))\n",
        "  epoch_cost_arr.append(np.average(cost_arr))\n",
        "  cost_arr = []\n",
        "\n",
        "plt.plot(epoch_cost_arr)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "4wBo_xup6XHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "test_results = []\n",
        "\n",
        "for i in range(0,x_test.shape[0]):\n",
        "  input_layer = x_test[i].reshape(784)\n",
        "\n",
        "  hidden_layer_z = np.dot(input_weights,input_layer)+hidden_biases\n",
        "  hidden_layer_activation = sigmoid(hidden_layer_z)\n",
        "\n",
        "  y_hat_z = np.dot(output_weights,hidden_layer_activation)+output_biases\n",
        "  y_hat = sigmoid(y_hat_z)\n",
        "\n",
        "  y = y_test[i]\n",
        "\n",
        "  test_results.append((np.argmax(y_hat),y))\n",
        "\n",
        "print(f'{sum(int(x == y) for (x, y) in test_results)/y_test.shape[0]*100}% accuracy')"
      ],
      "metadata": {
        "id": "zRfqTT9XSlm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}